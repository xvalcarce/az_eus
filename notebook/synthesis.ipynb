{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76ed5ce4-e966-4479-8158-5925bc3b37d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import shutil\n",
    "import configparser\n",
    "import optax\n",
    "import qujax\n",
    "import chex\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from functools import partial\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "from chex import dataclass\n",
    "\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "from core.memory.replay_memory import EpisodeReplayBuffer\n",
    "from core.networks.azresnet import AZResnet, AZResnetConfig\n",
    "from core.networks.aztransformer import AZTransformer, AZTransformerConfig\n",
    "from core.networks.azresnettransformer import AZResnetTransformer, AZResnetTransformerConfig\n",
    "from core.networks.azmlp import AZMLP, AZMLPConfig\n",
    "from core.evaluators.alphazero import AlphaZero\n",
    "from core.evaluators.mcts.weighted_mcts import MCTS\n",
    "from core.evaluators.mcts.action_selection import PUCTSelector\n",
    "from core.evaluators.evaluation_fns import make_nn_eval_fn, make_nn_eval_fn_no_params_callable\n",
    "from core.training.train import Trainer, TrainLoopOutput\n",
    "from core.training.loss_fns import az_default_loss_fn\n",
    "from core.types import StepMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2986878a-4cec-4421-b611-423a67b7269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide some pre-trained agents\n",
    "\n",
    "# 3qubits, all2all, {H,T,CX}\n",
    "# load_dir = \"data/3q_a2a/\"\n",
    "# 3qubits+1ancilla, {H,T,CX}\n",
    "load_dir = \"./data/3q_1a/\"\n",
    "# 2qubits+1ancilla\n",
    "# load_dir = \"./data/2q_1a\"\n",
    "\n",
    "# Step to restore, None restores the latest training step\n",
    "STEP = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd3060f1-dbf1-48f9-8fe3-50c40b1ea9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load agent config\n",
    "abs_load_dir = str(Path(os.getcwd()).parent.absolute())+\"/\"+load_dir\n",
    "config = configparser.ConfigParser()\n",
    "config.read(abs_load_dir+\"config.ini\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee92ff5a-9542-4adf-908b-e8125716ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use quantum_compilation environment config file\n",
    "import quantum_compilation as q\n",
    "path_qc = q.__file__.split(\"/\")[:-2]\n",
    "path_qc.append(\"config.ini\")\n",
    "path_qc = \"/\".join(path_qc)\n",
    "shutil.copyfile(abs_load_dir+\"qc_config.ini\",path_qc)\n",
    "reload(q)\n",
    "import quantum_compilation.quantumcompilation as qc\n",
    "#from quantum_compilation.config import DEPTH, N_QUBITS, MAX_TARGET_DEPTH, DIM_OBS, FID_RENORM, FIDELITY\n",
    "from quantum_compilation.quantumcompilation import DEPTH, N_QUBITS, MAX_TARGET_DEPTH, DIM_OBS, FID_RENORM, FIDELTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f425a499-20c9-45f1-b19b-fe060fad8a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum compilation environment\n",
    "env = qc.QuantumCompilation()\n",
    "max_steps = DEPTH\n",
    "M_TARGET_DEPTH = int(config[\"environment\"][\"init_m_target_depth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c429a85-54c3-4f9d-9b34-e51bd138629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy target unitary\n",
    "mat = qujax.get_params_to_unitarytensor_func(['CX'],[[0,1]],[[]],N_QUBITS)\n",
    "#mat = qujax.get_params_to_unitarytensor_func(['CCX'],[[0,1,2]],[[]],qc.N_QUBITS)\n",
    "TARGET_V = mat().reshape(DIM_OBS,DIM_OBS).astype(jnp.complex64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb8bba90-bfe4-4167-9786-21b7eb6ffbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment dynamics functions\n",
    "def _init_fn(key,v=TARGET_V):\n",
    "    state = qc._init_u(v,MAX_TARGET_DEPTH)\n",
    "    observation = env.observe(state)\n",
    "    state = state.replace(observation=observation)\n",
    "    metadata = StepMetadata(\n",
    "        rewards = state.rewards,\n",
    "        terminated = state.terminated,\n",
    "        action_mask = state.legal_action_mask,\n",
    "        cur_player_id = state.current_player,\n",
    "        step=state._step_count\n",
    "    )\n",
    "    return state, metadata\n",
    "\n",
    "def step_fn(state, action):\n",
    "    state = env.step(state, action)\n",
    "    metadata = StepMetadata(\n",
    "        rewards = state.rewards,\n",
    "        terminated = state.terminated,\n",
    "        action_mask = state.legal_action_mask,\n",
    "        cur_player_id = state.current_player,\n",
    "        step = state._step_count\n",
    "    )\n",
    "    return state, metadata\n",
    "\n",
    "def print_circuit(gates, l):\n",
    "    out = \" ; \".join([qc.GATE_NAMES[g] for g in gates.tolist()[:l]])\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3779369c-6a1a-4834-b5c6-352bbe3cc455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent\n",
    "arch = config.get(\"neuralnetwork\", \"architecture\") \n",
    "if arch == \"Resnet\":\n",
    "    network = AZResnet\n",
    "    networkconfig = AZResnetConfig\n",
    "    kernel_size=int(config[\"neuralnetwork\"][\"kernel_size\"])\n",
    "    nn = network(networkconfig(\n",
    "        policy_head_out_size=env.num_actions,\n",
    "        num_blocks=int(config[\"neuralnetwork\"][\"num_blocks\"]),\n",
    "        num_channels=int(config[\"neuralnetwork\"][\"num_channels\"]),\n",
    "        num_policy_channels=int(config[\"neuralnetwork\"][\"num_policy_channels\"]),\n",
    "        num_value_channels=int(config[\"neuralnetwork\"][\"num_value_channels\"]),\n",
    "        kernel_size=kernel_size,\n",
    "        kernel_size_value=config.getint(\"neuralnetwork\",\"kernel_size_value\", fallback=kernel_size),\n",
    "        kernel_size_policy=config.getint(\"neuralnetwork\",\"kernel_size\", fallback=kernel_size),\n",
    "        batch_norm_momentum=config.getfloat(\"neuralnetwork\",\"batch_norm_momentum\"),\n",
    "    ))\n",
    "elif arch == \"ResnetTransformer\":\n",
    "    network = AZResnetTransformer\n",
    "    networkconfig = AZResnetTransformerConfig\n",
    "    nn = network(networkconfig(\n",
    "        policy_head_out_size=env.num_actions,\n",
    "        num_blocks=int(config[\"neuralnetwork\"][\"num_blocks\"]),\n",
    "        num_channels=int(config[\"neuralnetwork\"][\"num_channels\"]),\n",
    "        num_policy_channels=int(config[\"neuralnetwork\"][\"num_policy_channels\"]),\n",
    "        num_value_channels=int(config[\"neuralnetwork\"][\"num_value_channels\"]),\n",
    "        kernel_size=int(config[\"neuralnetwork\"][\"kernel_size\"]),\n",
    "        batch_norm_momentum=config.getfloat(\"neuralnetwork\",\"batch_norm_momentum\"),\n",
    "        num_transformer_heads=config.getint(\"neuralnetwork\",\"num_transformer_heads\"),\n",
    "        transformer_mlp_dim=config.getint(\"neuralnetwork\",\"transformer_mlp_dim\"),\n",
    "        transformer_embed_dim=config.getint(\"neuralnetwork\",\"transformer_embed_dim\"),\n",
    "    ))\n",
    "elif arch == \"Transformer\":\n",
    "    network = AZTransformer\n",
    "    networkconfig = AZTransformerConfig\n",
    "    nn = network(networkconfig(\n",
    "        policy_head_out_size=env.num_actions,\n",
    "        num_blocks=int(config[\"neuralnetwork\"][\"num_blocks\"]),\n",
    "        num_heads=config.getint(\"neuralnetwork\",\"num_heads\"),\n",
    "        mlp_dim=config.getint(\"neuralnetwork\",\"mlp_dim\"),\n",
    "        embed_dim=config.getint(\"neuralnetwork\",\"embed_dim\"),\n",
    "    ))\n",
    "elif arch == \"MLP\":\n",
    "    network = AZMLP\n",
    "    networkconfig = AZMLPConfig\n",
    "    nn = network(networkconfig(\n",
    "        policy_head_out_size=env.num_actions,\n",
    "        width = config.getint(\"neuralnetwork\",\"width\"),\n",
    "        depth_common = config.getint(\"neuralnetwork\",\"depth_common\"),\n",
    "        depth_phead = config.getint(\"neuralnetwork\",\"depth_phead\"),\n",
    "        depth_vhead = config.getint(\"neuralnetwork\",\"depth_vhead\"),\n",
    "        use_batch_norm = config.getboolean(\"neuralnetwork\",\"use_batch_norm\", fallback=True),\n",
    "        batch_norm_momentum = config.getfloat(\"neuralnetwork\",\"batch_norm_momentum\"),\n",
    "        dropout_rate = config.getfloat(\"neuralnetwork\",\"dropout_rate\"),\n",
    "    ))\n",
    "else:\n",
    "    raise TypeError(\"Network not supported\")\n",
    "\n",
    "replay_memory = EpisodeReplayBuffer(capacity=int(config[\"replay_memory\"][\"capacity\"]))\n",
    "\n",
    "def state_to_nn_input(state):\n",
    "    # pgx does this for us with state.observation!\n",
    "    return state.observation\n",
    "\n",
    "## AlphaZero and MCTS players\n",
    "\n",
    "# Define AlphaZero evaluator for self-play\n",
    "alphazero = AlphaZero(MCTS)(\n",
    "    eval_fn=make_nn_eval_fn(nn, state_to_nn_input),\n",
    "    num_iterations=int(config[\"alphazero_evaluation\"][\"num_iterations\"]),\n",
    "    max_nodes=int(config[\"alphazero_evaluation\"][\"max_nodes\"]),\n",
    "    dirichlet_alpha=float(config[\"alphazero_selfplay\"][\"dirichlet_alpha\"]),\n",
    "    dirichlet_epsilon=float(config[\"alphazero_selfplay\"][\"dirichlet_epsilon\"]),\n",
    "    temperature=float(config[\"alphazero_selfplay\"][\"temperature\"]),\n",
    "    branching_factor=env.num_actions,\n",
    "    action_selector=PUCTSelector(c=float(config[\"alphazero_selfplay\"][\"puct_c\"])),\n",
    "    discount=float(config[\"alphazero_selfplay\"][\"discount\"]),\n",
    ")\n",
    "\n",
    "# Define AlphaZero evaluator for evaluation games\n",
    "alphazero_test = AlphaZero(MCTS)(\n",
    "    eval_fn=make_nn_eval_fn(nn, state_to_nn_input),\n",
    "    num_iterations=1000,\n",
    "    max_nodes=1000,\n",
    "    temperature=0.4,\n",
    "    dirichlet_alpha=config.getfloat(\"alphazero_evaluation\", \"dirichlet_alpha\", fallback=1.0),\n",
    "    dirichlet_epsilon=float(config[\"alphazero_evaluation\"][\"dirichlet_epsilon\"]),\n",
    "    branching_factor=env.num_actions,\n",
    "    action_selector=PUCTSelector(c=float(config[\"alphazero_evaluation\"][\"puct_c\"])),\n",
    "    discount=float(config[\"alphazero_evaluation\"][\"discount\"]),\n",
    ")\n",
    "\n",
    "# Define AlphaZero evaluator for evaluation games\n",
    "alphazero_deterministic = AlphaZero(MCTS)(\n",
    "    eval_fn=make_nn_eval_fn(nn, state_to_nn_input),\n",
    "    num_iterations=1000,\n",
    "    max_nodes=1000,\n",
    "    temperature=0.0,\n",
    "    dirichlet_alpha=config.getfloat(\"alphazero_evaluation\", \"dirichlet_alpha\", fallback=1.0),\n",
    "    dirichlet_epsilon=float(config[\"alphazero_evaluation\"][\"dirichlet_epsilon\"]),\n",
    "    branching_factor=env.num_actions,\n",
    "    action_selector=PUCTSelector(c=float(config[\"alphazero_evaluation\"][\"puct_c\"])),\n",
    "    discount=float(config[\"alphazero_evaluation\"][\"discount\"]),\n",
    ")\n",
    "\n",
    "def fid_eval(obs):\n",
    "    obs = obs[0,0]+obs[1,1]*1j\n",
    "    f = jnp.square(jnp.abs(obs.trace()))/FID_RENORM > FIDELTY\n",
    "    return jnp.ones((1,env.num_actions)), jnp.array([f],dtype=jnp.float32)\n",
    "\n",
    "fid_baseline_eval_fn = make_nn_eval_fn_no_params_callable(fid_eval, state_to_nn_input)\n",
    "\n",
    "mcts_baseline = AlphaZero(MCTS)(\n",
    "        eval_fn=fid_baseline_eval_fn,\n",
    "        num_iterations = 400,\n",
    "        max_nodes = 1000,\n",
    "        branching_factor = env.num_actions,\n",
    "        action_selector = PUCTSelector(c=float(config[\"alphazero_evaluation\"][\"puct_c\"])),\n",
    "        temperature = 0.0,\n",
    "        discount = 1.0\n",
    ")\n",
    "\n",
    "mcts_baseline_stochastic = AlphaZero(MCTS)(\n",
    "        eval_fn=fid_baseline_eval_fn,\n",
    "        num_iterations = 400,\n",
    "        max_nodes = 1000,\n",
    "        branching_factor = env.num_actions,\n",
    "        action_selector = PUCTSelector(c=float(config[\"alphazero_evaluation\"][\"puct_c\"])),\n",
    "        temperature = 0.6,\n",
    "        discount = 1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e725f2e3-af60-4a61-a982-4fbf501d9727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "batch_size = int(config[\"trainer\"][\"batch_size\"])\n",
    "train_batch_size = int(config[\"trainer\"][\"train_batch_size\"])\n",
    "warmup_steps = int(config[\"trainer\"][\"warmup_steps\"])\n",
    "collection_steps_per_epoch = int(config[\"trainer\"][\"collection_steps_per_epoch\"])\n",
    "train_steps_per_epoch = batch_size * collection_steps_per_epoch // train_batch_size\n",
    "\n",
    "opt = config.get(\"trainer\", \"optimizer\") \n",
    "if opt ==\"sgd\":\n",
    "    optimizer = optax.sgd\n",
    "elif opt == \"adam\":\n",
    "    optimizer = optax.adam\n",
    "elif opt == \"adamw\":\n",
    "    optimizer = optax.adamw\n",
    "else:\n",
    "    raise TypeError(\"Not a valid optimizer (sgd, adam, adamw)\")\n",
    "\n",
    "\n",
    "# Dummy trainer\n",
    "trainer = Trainer(\n",
    "    batch_size=8,\n",
    "    train_batch_size=8,\n",
    "    warmup_steps=warmup_steps,\n",
    "    collection_steps_per_epoch=8,\n",
    "    train_steps_per_epoch=8,\n",
    "    nn=nn,\n",
    "    loss_fn=partial(az_default_loss_fn, l2_reg_lambda=float(config[\"trainer\"][\"l2_reg_lambda\"])),\n",
    "    optimizer=optimizer(float(config[\"trainer\"][\"optimizer_lr\"])),\n",
    "    evaluator=alphazero,\n",
    "    memory_buffer=replay_memory,\n",
    "    max_episode_steps=max_steps,\n",
    "    env_step_fn=step_fn,\n",
    "    env_init_fn=_init_fn,\n",
    "    state_to_nn_input_fn=state_to_nn_input,\n",
    "    testers=[],\n",
    "    evaluator_test=alphazero_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9f3febc-48f0-4649-9ee2-f9cf0fb42328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load agent from saved data\n",
    "def loading() -> TrainLoopOutput:\n",
    "    with open(abs_load_dir+'collection.pickle', 'rb') as f:\n",
    "        collection_state = pickle.load(f)\n",
    "    with open(abs_load_dir+'test_states.pickle', 'rb') as f:\n",
    "        # Serialize and save the object to the file\n",
    "        test_states = pickle.load(f)\n",
    "    with open(abs_load_dir+'cur_epoch.pickle', 'rb') as f:\n",
    "        # Serialize and save the object to the file\n",
    "        cur_epoch = pickle.load(f)\n",
    "\n",
    "    # Restore backup train state\n",
    "    # Copy backed up checkpoint to ckpt_dir\n",
    "    shutil.copytree(abs_load_dir+str(cur_epoch-1),trainer.ckpt_dir+\"/\"+str(cur_epoch-1),dirs_exist_ok=True)\n",
    "    # Load train_state\n",
    "    train_state = trainer.load_train_state_from_checkpoint(trainer.ckpt_dir, cur_epoch-1)\n",
    "    \n",
    "    # Build a TrainLoopOutput\n",
    "    init_state = TrainLoopOutput(\n",
    "        collection_state=collection_state,\n",
    "        train_state=train_state,\n",
    "        test_states=test_states,\n",
    "        cur_epoch=cur_epoch)\n",
    "    return init_state\n",
    "\n",
    "def reshape_nested_dict(d):\n",
    "    if isinstance(d, dict):\n",
    "        return {k: reshape_nested_dict(v) for k, v in d.items()}\n",
    "    elif isinstance(d, jnp.ndarray) and d.shape[0] == 1:  \n",
    "        return d.squeeze(axis=0)  # Remove the first dimension\n",
    "    else:\n",
    "        return d  # Return unchanged if not a JAX array or has different shape\n",
    "\n",
    "# abstract tree for params restoring\n",
    "key = jax.random.PRNGKey(0)\n",
    "init_key, key = jax.random.split(key)\n",
    "init_keys = jnp.tile(init_key[None], (trainer.num_devices, 1))\n",
    "dummy_state = trainer.init_train_state(init_keys)\n",
    "\n",
    "# restore nn params from latest training step\n",
    "ck = ocp.CheckpointManager(abs_load_dir)\n",
    "try:\n",
    "    s = ck.restore(ck.latest_step(), args=ocp.args.StandardRestore(dummy_state, strict=False))\n",
    "except:\n",
    "    s = ck.restore(ck.latest_step(), items=dummy_state, restore_kwargs={'strict': False})\n",
    "restored_step = ck.latest_step() if STEP == None else STEP\n",
    "s = ck.restore(restored_step, args=ocp.args.StandardRestore(dummy_state, strict=False))\n",
    "variables = {'params': s.params, 'batch_stats': s.batch_stats}\n",
    "variables = reshape_nested_dict(variables) # squeeze num_devices\n",
    "\n",
    "def restore_variables(i):\n",
    "    try:\n",
    "        s = ck.restore(i, args=ocp.args.StandardRestore(dummy_state, strict=False))\n",
    "    except:\n",
    "        s = ck.restore(i, items=dummy_state, restore_kwargs={'strict': False})\n",
    "    s = ck.restore(i, args=ocp.args.StandardRestore(dummy_state, strict=False))\n",
    "    variables = {'params': s.params, 'batch_stats': s.batch_stats}\n",
    "    variables = reshape_nested_dict(variables) # squeeze num_devices\n",
    "    return variables\n",
    "\n",
    "all_variables = [restore_variables(i) for i in ck.all_steps()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a3f4871-a507-4df5-83bb-a7f7ad4a7da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AZ agent\n",
    "evaluator = alphazero\n",
    "env_state, metadata = _init_fn(key)\n",
    "eval_state = evaluator.init(template_embedding=env_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8106085f-127e-4293-9d0e-65c3a41c69b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single player class\n",
    "@dataclass(frozen=True)\n",
    "class SinglePlayerGameState:\n",
    "    \"\"\"Stores the state of a single-player game for two evaluators playing independently.\n",
    "    - `key`: rng\n",
    "    - `env_state`: The initial environment state.\n",
    "    - `env_state_metadata`: Metadata associated with the initial environment state.\n",
    "    - `eval_state_1`: The internal state of the first evaluator.\n",
    "    - `eval_state_2`: The internal state of the second evaluator.\n",
    "    - `completed_1`: Whether the first evaluator's game is completed.\n",
    "    - `completed_2`: Whether the second evaluator's game is completed.\n",
    "    - `outcome_1`: The final reward of the first evaluator.\n",
    "    - `outcome_2`: The final reward of the second evaluator.\n",
    "    \"\"\"\n",
    "    key: jax.random.PRNGKey\n",
    "    env_state: chex.ArrayTree\n",
    "    env_state_metadata: StepMetadata\n",
    "    eval_state: chex.ArrayTree\n",
    "    completed: bool\n",
    "    outcome: float\n",
    "\n",
    "# A game\n",
    "# @partial(jax.pmap, axis_name='p', static_broadcasted_argnums=(0,))\n",
    "def game_step(state: SinglePlayerGameState, _, params: chex.ArrayTree, env_step_fn=step_fn, evaluator=alphazero):\n",
    "    step_key, key = jax.random.split(state.key)\n",
    "    # Evaluate and take action\n",
    "    output = evaluator.evaluate(\n",
    "        key=step_key,\n",
    "        eval_state=state.eval_state,\n",
    "        env_state=state.env_state,\n",
    "        root_metadata=state.env_state_metadata,\n",
    "        params=params,\n",
    "        env_step_fn=env_step_fn\n",
    "    )\n",
    "    next_env_state, next_env_metadata = env_step_fn(state.env_state, output.action)\n",
    "    terminated = next_env_metadata.terminated\n",
    "    truncated = next_env_metadata.step > max_steps\n",
    "    completed = terminated | truncated\n",
    "    rewards = next_env_metadata.rewards\n",
    "    eval_state = jax.lax.cond(\n",
    "        completed,\n",
    "        lambda _: state.eval_state,\n",
    "        lambda _: evaluator.step(state.eval_state, output.action),\n",
    "        None\n",
    "    )\n",
    "    state = state.replace(\n",
    "            key=key,\n",
    "            env_state = next_env_state,\n",
    "            env_state_metadata = next_env_metadata,\n",
    "            eval_state = eval_state,\n",
    "            completed = completed,\n",
    "            outcome = rewards)\n",
    "    return state, state\n",
    "\n",
    "game_step_ = partial(game_step, params=variables, env_step_fn=step_fn, evaluator=alphazero_test)\n",
    "game_step_deterministic = partial(game_step, params=variables, env_step_fn=step_fn, evaluator=alphazero_deterministic)\n",
    "game_step_mcts = partial(game_step, params=None, env_step_fn=step_fn, evaluator=mcts_baseline)\n",
    "game_step_mcts_stochastic = partial(game_step, params=None, env_step_fn=step_fn, evaluator=mcts_baseline_stochastic)\n",
    "\n",
    "def game(key, state, max_steps=max_steps):\n",
    "    state = state.replace(key=key)\n",
    "    state, collection_state = jax.lax.scan(\n",
    "            game_step_,\n",
    "            init=state,\n",
    "            xs=None,\n",
    "            length=max_steps\n",
    "            )\n",
    "    return collection_state\n",
    "\n",
    "def game_deterministic(key, state, max_steps=max_steps):\n",
    "    state = state.replace(key=key)\n",
    "    state, collection_state = jax.lax.scan(\n",
    "            game_step_deterministic,\n",
    "            init=state,\n",
    "            xs=None,\n",
    "            length=max_steps\n",
    "            )\n",
    "    return collection_state\n",
    "\n",
    "def game_mcts(key, state, max_steps=max_steps):\n",
    "    state = state.replace(key=key)\n",
    "    state, collection_state = jax.lax.scan(\n",
    "            game_step_mcts,\n",
    "            init=state,\n",
    "            xs=None,\n",
    "            length=max_steps\n",
    "            )\n",
    "    return collection_state\n",
    "\n",
    "def game_mcts_stochastic(key, state, max_steps=max_steps):\n",
    "    state = state.replace(key=key)\n",
    "    state, collection_state = jax.lax.scan(\n",
    "            game_step_mcts_stochastic,\n",
    "            init=state,\n",
    "            xs=None,\n",
    "            length=max_steps\n",
    "            )\n",
    "    return collection_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97ecf9a3-a9e4-425d-ac42-de28f621ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile(unitary='CX',locs=[0,1],run=10,batch_run=10,key=jax.random.PRNGKey(0),deterministic_run=False, max_steps=max_steps, hotstart=[]):\n",
    "    \"\"\"\n",
    "    Synthesize a target quantum unitary into a quantum circuit.\n",
    "\n",
    "    This function builds a target unitary from a specified quantum gate and qubit\n",
    "    locations, initializes a reinforcement-learning environment, optionally applies\n",
    "    a sequence of pre-defined gates (hotstart), and then searches for a circuit that\n",
    "    exactly reproduces the target unitary.\n",
    "\n",
    "    Two execution modes are supported:\n",
    "    - Deterministic mode: a single zero-temperature rollout.\n",
    "    - Stochastic mode: multiple batched Monte Carlo rollouts with temperature.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    unitary : Any, optional\n",
    "        Name of the quantum gate to compile (e.g., 'CX', 'H'), supported by qujax.\n",
    "        Alternatively, you can provied a custom gate (jnp.array).\n",
    "        Default is 'CX'.\n",
    "        \n",
    "    locs : list[int], optional\n",
    "        Target qubit indices where the gate acts.\n",
    "        Default is [0, 1].\n",
    "\n",
    "    run : int, optional\n",
    "        Total number of stochastic runs to perform.\n",
    "        Must be divisible by `batch_run`.\n",
    "        Default is 10.\n",
    "\n",
    "    batch_run : int, optional\n",
    "        Number of trajectories evaluated in parallel using `jax.vmap`.\n",
    "        Default is 10.\n",
    "\n",
    "    key : jax.random.PRNGKey, optional\n",
    "        Random key used for all stochastic sampling.\n",
    "        Default is `jax.random.PRNGKey(0)`.\n",
    "\n",
    "    deterministic_run : bool, optional\n",
    "        If True, runs a single deterministic (temperature=0) search.\n",
    "        If False, runs stochastic batched searches (temperature=1).\n",
    "        Default is False.\n",
    "\n",
    "    max_steps : int\n",
    "        Maximum number of steps allowed for each search trajectory.\n",
    "        This value is reduced internally by the length of `hotstart`.\n",
    "\n",
    "    hotstart : list, optional\n",
    "        A list of pre-applied actions (gates) used to initialize the circuit\n",
    "        before the search begins.\n",
    "        Default is an empty list.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if a valid circuit reproducing the target unitary is found.\n",
    "        False otherwise.\n",
    "\n",
    "    Side Effects\n",
    "    ------------\n",
    "    - Prints the target unitary being compiled.\n",
    "    - Prints runtime statistics.\n",
    "    - Prints the discovered circuit and its depth if successful.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - In deterministic mode, the first successful trajectory is returned.\n",
    "    - In stochastic mode, the shortest successful circuit across all batches\n",
    "      is selected.\n",
    "    \"\"\"\n",
    "    mat = qujax.get_params_to_unitarytensor_func([unitary],[locs],[[]],N_QUBITS)\n",
    "    target_v = mat().reshape(DIM_OBS,DIM_OBS).astype(jnp.complex64)\n",
    "    env_state, metadata = _init_fn(key,v=target_v)\n",
    "    max_steps = max_steps-len(hotstart)\n",
    "    for g in hotstart:\n",
    "        env_state, metadata = step_fn(env_state,g)\n",
    "    print(\"Compiling the unitary (dag):\")\n",
    "    print(env_state._target_unitary)\n",
    "\n",
    "    if deterministic_run:\n",
    "        #deterministic run (temp=0.)\n",
    "        eval_state = alphazero_deterministic.init(template_embedding=env_state)\n",
    "        init_state = SinglePlayerGameState(key=key, \n",
    "                                           env_state=env_state, \n",
    "                                           env_state_metadata=metadata, \n",
    "                                           eval_state=eval_state, \n",
    "                                           completed=jnp.array(False, dtype=jnp.bool_), \n",
    "                                           outcome=jnp.array([0.0], dtype=jnp.float32))\n",
    "        t = time.time()\n",
    "        sd = game_deterministic(key, init_state, max_steps)\n",
    "        print(f\"Runtime {round(time.time()-t,2)}\")\n",
    "        idx = jnp.where(sd.outcome == 1)\n",
    "        if idx[0].size == 0:\n",
    "            print(\"No circuit found deterministically.\")\n",
    "            return False\n",
    "        else:\n",
    "            len_c = idx[0][1]+len(hotstart)\n",
    "            print(f\"Circuit found deterministically with depth {len_c+1}:\")\n",
    "            print_circuit(sd.env_state._circuit[len_c],len_c+1)\n",
    "            return True\n",
    "\n",
    "    # stochastic runs (temp=1.)\n",
    "    eval_state = alphazero_test.init(template_embedding=env_state)\n",
    "    init_state = SinglePlayerGameState(key=key, \n",
    "                                       env_state=env_state, \n",
    "                                       env_state_metadata=metadata, \n",
    "                                       eval_state=eval_state, \n",
    "                                       completed=jnp.array(False, dtype=jnp.bool_), \n",
    "                                       outcome=jnp.array([0.0], dtype=jnp.float32))\n",
    "    gg = partial(game, state=init_state, max_steps=max_steps)\n",
    "    r = run//batch_run\n",
    "    t = time.time()\n",
    "    for ii in range(r):\n",
    "        key, _ = jax.random.split(key)\n",
    "        keys = jax.random.split(key, num=batch_run) # 10 is reasonnable for 8GB of VRAM\n",
    "        s = jax.vmap(gg)(keys)\n",
    "        # extract indicies, non zero values\n",
    "        idx = jnp.where(s.outcome == 1)\n",
    "        if idx[0].size != 0:\n",
    "            print(f\"Runtime {round(time.time()-t,2)}\")\n",
    "            # element with smallest len\n",
    "            min_c = jnp.argmin(idx[1])\n",
    "            id_c = idx[0][min_c] # idices of cicruit\n",
    "            len_c = idx[1][min_c]+len(hotstart) # length of circuit\n",
    "            print(f\"Circuit found with depth {len_c+1}:\")\n",
    "            print_circuit(s.env_state._circuit[id_c][len_c],len_c+1)\n",
    "            print(s.env_state._circuit[id_c][len_c])\n",
    "            return True\n",
    "    print(f\"Runtime {round(time.time()-t,2)}\")\n",
    "    print(\"No circuit found.\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ae43f32-d9a1-4b90-91bd-38e2f7715e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the unitary (dag):\n",
      "[[1.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j]\n",
      " [0.-0.j 1.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j]\n",
      " [0.-0.j 0.-0.j 1.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j]\n",
      " [0.-0.j 0.-0.j 0.-0.j 1.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j]\n",
      " [0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 1.-0.j 0.-0.j]\n",
      " [0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 1.-0.j]\n",
      " [0.-0.j 0.-0.j 0.-0.j 0.-0.j 1.-0.j 0.-0.j 0.-0.j 0.-0.j]\n",
      " [0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 1.-0.j 0.-0.j 0.-0.j]]\n",
      "Runtime 25.52\n",
      "Circuit found with depth 3:\n",
      "H_3 ; CX_(3)1 ; CX_(0)3\n",
      "[6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that everything works well with the synthesis of a simple CX gate (should work, even if the agent is not trained)\n",
    "compile(max_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24db1cab-5efb-44ca-959b-13b1d0a9aa91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the unitary (dag):\n",
      "[[1.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j]\n",
      " [0.-0.j 1.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j]\n",
      " [0.-0.j 0.-0.j 1.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j]\n",
      " [0.-0.j 0.-0.j 0.-0.j 1.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j]\n",
      " [0.-0.j 0.-0.j 0.-0.j 0.-0.j 1.-0.j 0.-0.j 0.-0.j 0.-0.j]\n",
      " [0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 1.-0.j 0.-0.j 0.-0.j]\n",
      " [0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-1.j 0.-0.j]\n",
      " [0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-1.j]]\n",
      "Runtime 3.37\n",
      "Circuit found deterministically with depth 7:\n",
      "H_3 ; T_3 ; CX_(0)3 ; Tdg_3 ; CX_(1)3 ; T_3 ; CX_(0)3\n"
     ]
    }
   ],
   "source": [
    "from gates import CS\n",
    "\n",
    "# Example of a synthesis workflow, here for the CS gate hotstarting a |T> state\n",
    "s = compile(unitary=CS,locs=[0,1],key=jax.random.PRNGKey(0),deterministic_run=True, max_steps=7, hotstart=[6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e61f1-5ede-4c0f-9fcc-3c2a15ad3fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the unitary (dag):\n",
      "[[1.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j]\n",
      " [0.-0.j 1.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j]\n",
      " [0.-0.j 0.-0.j 1.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j]\n",
      " [0.-0.j 0.-0.j 0.-0.j 1.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j]\n",
      " [0.-0.j 0.-0.j 0.-0.j 0.-0.j 1.-0.j 0.-0.j 0.-0.j 0.-0.j]\n",
      " [0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 1.-0.j 0.-0.j 0.-0.j]\n",
      " [0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 1.-0.j]\n",
      " [0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 0.-0.j 1.-0.j 0.-0.j]]\n"
     ]
    }
   ],
   "source": [
    "# Example of a synthesis workflow, here for the CCX gate\n",
    "# Start with a deterministic run\n",
    "s = compile(unitary='CCX',locs=[0,1,2],key=jax.random.PRNGKey(0),deterministic_run=True, max_steps=17)\n",
    "# If it fails, run stochastic runs\n",
    "if not s:\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    s = compile(unitary='CCX',locs=[0,1,2],key=key,deterministic_run=False, run=100, max_steps=17)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (az_eus)",
   "language": "python",
   "name": "az_eus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
