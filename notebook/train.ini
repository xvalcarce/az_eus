[environment]
; average target depth when starting the training loop
init_m_target_depth = 5
; final average target depth 
final_m_target_depth = 30
; increment of the average target depth after every epoch
target_depth_increment = 1

[neuralnetwork]
architecture = Resnet
num_blocks = 5
num_channels = 64
num_policy_channels = 32
num_value_channels = 8
batch_norm_momentum = 0.9
kernel_size = 3
kernel_policy_size = 1
kernel_value_size = 1

; architecture = Transformer
; num_blocks = 6
; num_heads = 8
; embed_dim = 128
; mlp_dim = 512

; architecture = MLP
; width = 64
; depth_common = 5
; depth_phead = 2
; depth_vhead = 2
; use_batch_norm = True
; batch_norm_momentum = 0.1
; dropout_rate = 0.2

; architecture = ResnetTransformer 
; num_blocks = 5
; num_channels = 32
; num_policy_channels = 32
; num_value_channels = 32
; batch_norm_momentum = 0.9
; kernel_size = 3
; num_transformer_heads = 8
; transformer_mlp_dim = 64
; transformer_embed_dim = 32

; architecture = VisionTransformer
; resnet_num_blocks = 2
; resnet_num_channels = 64
; transformer_num_heads = 4
; transformer_num_layers = 1
; transformer_mlp_dim = 256
; transformer_patches_size = 3
; transformer_hidden_size = 128
; batch_norm_momentum = 1.0
; kernel_size = 3

[replay_memory]
; number of game to store in memory
capacity = 32768

[alphazero_selfplay]
use_dynamic_temperature = True
d_temperature = [[5, 1.0], [10,0.6], [15,0.4]] 
num_iterations = 200
max_nodes = 1_000
dirichlet_alpha = 1.0
dirichlet_epsilon = 0.25
temperature = 1.0
puct_c = 2.0
; single player game
discount = 1.0

[alphazero_evaluation]
use_dynamic_temperature = False
num_iterations = 200
max_nodes = 1_000
dirichlet_epsilon = 0.05
temperature = 0.2
puct_c = 2.0
; single player game
discount = 1.0

[trainer]
; number of games to collect in parallel
batch_size = 64
; total number of steps per epoch is batch_size*collection_steps_per_epoch* mean target_depth
collection_steps_per_epoch = 32
; number of games to learn from per training steps
train_batch_size = 64
; before epoch starts, warmup_steps*batch_size games are collected
warmup_steps = 16
; optimizer (adam or adamw or sgd)
optimizer = adam
; learning rate
optimizer_lr = 0.001
; l2 regularization
l2_reg_lambda = 0.0001
; number of epochs (per target_depth)
num_epochs = 5
; extra epochs after increasing target_depth to maximum
extra_epochs = 0
; number of episodes to test the updated agent on
test_num_episodes = 100

[saving]
; directory to save checkpoint
bckp_dir = ../data/resnet/all2all
; make a subdirectory with the datetime at run
use_date = True