{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11eb3c27-c46c-496f-9249-c9eb210012c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pkgs\n",
    "import os\n",
    "import ast\n",
    "import datetime\n",
    "import pickle\n",
    "import shutil\n",
    "import configparser\n",
    "import optax\n",
    "from functools import partial\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Load envionment pkg\n",
    "import quantum_compilation as q\n",
    "import quantum_compilation.quantumcompilation as qc\n",
    "\n",
    "# Load AZ pkg\n",
    "from core.memory.replay_memory import EpisodeReplayBuffer\n",
    "from core.networks.azresnet import AZResnet, AZResnetConfig\n",
    "from core.networks.azresnettransformer import AZResnetTransformer, AZResnetTransformerConfig\n",
    "from core.networks.aztransformer import AZTransformer, AZTransformerConfig\n",
    "from core.networks.azvit import AZVisionTransformer, AZVisionTransformerConfig\n",
    "from core.networks.azmlp import AZMLP, AZMLPConfig\n",
    "from core.evaluators.alphazero import AlphaZero\n",
    "from core.evaluators.mcts.weighted_mcts import WeightedMCTS, MCTS\n",
    "from core.evaluators.mcts.scheduled_temp_mcts import ScheduledTemperatureMCTS\n",
    "from core.evaluators.mcts.action_selection import PUCTSelector\n",
    "from core.evaluators.evaluation_fns import make_nn_eval_fn, make_nn_eval_fn_no_params_callable\n",
    "from core.testing.two_player_tester import TwoPlayerTester\n",
    "from core.testing.single_player_tester import SinglePlayerTester\n",
    "from core.training.train import Trainer, TrainLoopOutput\n",
    "from core.training.loss_fns import az_default_loss_fn\n",
    "from core.types import StepMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f01f4c-800e-4484-8438-cf3f1815cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"./train.ini\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76735a13-98e2-497c-89d5-ecdcdc80d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum compilation environment\n",
    "env = qc.QuantumCompilation()\n",
    "mat_size = qc.DIM_OBS**2\n",
    "max_steps = qc.DEPTH\n",
    "M_TARGET_DEPTH = int(config[\"environment\"][\"init_m_target_depth\"])\n",
    "\n",
    "# define environment dynamics functions\n",
    "def step_fn(state, action):\n",
    "    state = env.step(state, action)\n",
    "    metadata = StepMetadata(\n",
    "        rewards = state.rewards,\n",
    "        terminated = state.terminated,\n",
    "        action_mask = state.legal_action_mask,\n",
    "        cur_player_id = state.current_player,\n",
    "        step = state._step_count\n",
    "    )\n",
    "    return state, metadata\n",
    "\n",
    "def _init_fn(key):\n",
    "    state = env._init(key,m_target_depth=M_TARGET_DEPTH)\n",
    "    observation = env.observe(state)\n",
    "    state = state.replace(observation=observation)\n",
    "    metadata = StepMetadata(\n",
    "        rewards = state.rewards,\n",
    "        terminated = state.terminated,\n",
    "        action_mask = state.legal_action_mask,\n",
    "        cur_player_id = state.current_player,\n",
    "        step=state._step_count\n",
    "    )\n",
    "    return state, metadata\n",
    "\n",
    "def state_to_nn_input(state):\n",
    "    # pgx does this for us with state.observation!\n",
    "    return state.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ac26461-255e-406a-92d7-72dfe9974e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network\n",
    "arch = config.get(\"neuralnetwork\", \"architecture\") \n",
    "if arch == \"Resnet\":\n",
    "    network = AZResnet\n",
    "    networkconfig = AZResnetConfig\n",
    "    kernel_size = config.getint(\"neuralnetwork\",\"kernel_size\")\n",
    "    nn = network(networkconfig(\n",
    "        policy_head_out_size=env.num_actions,\n",
    "        num_blocks=int(config[\"neuralnetwork\"][\"num_blocks\"]),\n",
    "        num_channels=int(config[\"neuralnetwork\"][\"num_channels\"]),\n",
    "        num_policy_channels=int(config[\"neuralnetwork\"][\"num_policy_channels\"]),\n",
    "        num_value_channels=int(config[\"neuralnetwork\"][\"num_value_channels\"]),\n",
    "        kernel_size=int(config[\"neuralnetwork\"][\"kernel_size\"]),\n",
    "        kernel_size_value=config.getint(\"neuralnetwork\",\"kernel_size_value\",fallback=kernel_size),\n",
    "        kernel_size_policy=config.getint(\"neuralnetwork\",\"kernel_size_policy\",fallback=kernel_size),\n",
    "        batch_norm_momentum=config.getfloat(\"neuralnetwork\",\"batch_norm_momentum\"),\n",
    "    ))\n",
    "elif arch == \"ResnetTransformer\":\n",
    "    network = AZResnetTransformer\n",
    "    networkconfig = AZResnetTransformerConfig\n",
    "    nn = network(networkconfig(\n",
    "        policy_head_out_size=env.num_actions,\n",
    "        num_blocks=int(config[\"neuralnetwork\"][\"num_blocks\"]),\n",
    "        num_channels=int(config[\"neuralnetwork\"][\"num_channels\"]),\n",
    "        num_policy_channels=int(config[\"neuralnetwork\"][\"num_policy_channels\"]),\n",
    "        num_value_channels=int(config[\"neuralnetwork\"][\"num_value_channels\"]),\n",
    "        kernel_size=int(config[\"neuralnetwork\"][\"kernel_size\"]),\n",
    "        batch_norm_momentum=config.getfloat(\"neuralnetwork\",\"batch_norm_momentum\"),\n",
    "        num_transformer_heads=config.getint(\"neuralnetwork\",\"num_transformer_heads\"),\n",
    "        transformer_mlp_dim=config.getint(\"neuralnetwork\",\"transformer_mlp_dim\"),\n",
    "        transformer_embed_dim=config.getint(\"neuralnetwork\",\"transformer_embed_dim\"),\n",
    "    ))\n",
    "elif arch == \"MLP\":\n",
    "    network = AZMLP\n",
    "    networkconfig = AZMLPConfig\n",
    "    nn = network(networkconfig(\n",
    "        policy_head_out_size=env.num_actions,\n",
    "        width = config.getint(\"neuralnetwork\",\"width\"),\n",
    "        depth_common = config.getint(\"neuralnetwork\",\"depth_common\"),\n",
    "        depth_phead = config.getint(\"neuralnetwork\",\"depth_phead\"),\n",
    "        depth_vhead = config.getint(\"neuralnetwork\",\"depth_vhead\"),\n",
    "        use_batch_norm = config.getboolean(\"neuralnetwork\",\"use_batch_norm\", fallback=True),\n",
    "        batch_norm_momentum = config.getfloat(\"neuralnetwork\",\"batch_norm_momentum\"),\n",
    "        dropout_rate = config.getfloat(\"neuralnetwork\",\"dropout_rate\"),\n",
    "    ))\n",
    "elif arch == \"VisionTransformer\":\n",
    "    network = AZVisionTransformer\n",
    "    networkconfig = AZVisionTransformerConfig\n",
    "    nn = network(networkconfig(\n",
    "        policy_head_out_size=env.num_actions,\n",
    "        resnet_num_blocks=config.getint(\"neuralnetwork\", \"resnet_num_blocks\"),\n",
    "        resnet_num_channels=config.getint(\"neuralnetwork\", \"resnet_num_channels\"),\n",
    "        transformer_num_heads=config.getint(\"neuralnetwork\", \"transformer_num_heads\"),\n",
    "        transformer_num_layers=config.getint(\"neuralnetwork\", \"transformer_num_layers\"),\n",
    "        transformer_mlp_dim=config.getint(\"neuralnetwork\", \"transformer_mlp_dim\"),\n",
    "        transformer_patches_size=config.getint(\"neuralnetwork\", \"transformer_patches_size\"),\n",
    "        transformer_hidden_size=config.getint(\"neuralnetwork\", \"transformer_hidden_size\"),\n",
    "        batch_norm_momentum=config.getfloat(\"neuralnetwork\", \"batch_norm_momentum\"),\n",
    "        kernel_size=config.getint(\"neuralnetwork\", \"kernel_size\")\n",
    "    ))\n",
    "elif arch == \"Transformer\":\n",
    "    network = AZTransformer\n",
    "    networkconfig = AZTransformerConfig\n",
    "    nn = network(networkconfig(\n",
    "        policy_head_out_size=env.num_actions,\n",
    "        num_blocks=config.getint(\"neuralnetwork\", \"num_blocks\"),\n",
    "        num_heads=config.getint(\"neuralnetwork\", \"num_heads\"),\n",
    "        mlp_dim=config.getint(\"neuralnetwork\", \"mlp_dim\"),\n",
    "        embed_dim=config.getint(\"neuralnetwork\", \"embed_dim\"),\n",
    "    ))\n",
    "    # Tokenized matrix cell entry\n",
    "    def state_to_nn_input(state):\n",
    "        obs = state.observation\n",
    "        return obs.transpose().reshape(mat_size,2)\n",
    "else:\n",
    "    raise TypeError(\"Network not supported\")\n",
    "\n",
    "replay_memory = EpisodeReplayBuffer(capacity=int(config[\"replay_memory\"][\"capacity\"]))\n",
    "\n",
    "def az_factory(az_type: str, **kwargs):\n",
    "    if config.getboolean(az_type,\"use_dynamic_temperature\",fallback=False):\n",
    "        config_d_temp = ast.literal_eval(config.get(az_type,\"d_temperature\"))\n",
    "        d_temp = []\n",
    "        for c in config_d_temp:\n",
    "            d_temp += [c[1]]*c[0]       # Ensure arg2 is provided for Foobar\n",
    "        d_temp = jnp.array(d_temp)\n",
    "        return AlphaZero(ScheduledTemperatureMCTS)(**kwargs, d_temperature=d_temp)\n",
    "    else:\n",
    "        return AlphaZero(MCTS)(**kwargs)\n",
    "\n",
    "# Define AlphaZero evaluator for self-play\n",
    "alphazero = az_factory(\"alphazero_selfplay\",\n",
    "    eval_fn=make_nn_eval_fn(nn, state_to_nn_input),\n",
    "    num_iterations=int(config[\"alphazero_selfplay\"][\"num_iterations\"]),\n",
    "    max_nodes=int(config[\"alphazero_selfplay\"][\"max_nodes\"]),\n",
    "    dirichlet_alpha=float(config[\"alphazero_selfplay\"][\"dirichlet_alpha\"]),\n",
    "    dirichlet_epsilon=float(config[\"alphazero_selfplay\"][\"dirichlet_epsilon\"]),\n",
    "    temperature=float(config[\"alphazero_selfplay\"][\"temperature\"]),\n",
    "    branching_factor=env.num_actions,\n",
    "    action_selector=PUCTSelector(c=float(config[\"alphazero_selfplay\"][\"puct_c\"])),\n",
    "    discount=float(config[\"alphazero_selfplay\"][\"discount\"]),\n",
    ")\n",
    "\n",
    "# Define AlphaZero evaluator for evaluation games\n",
    "alphazero_test = az_factory(\"alphazero_evaluation\",\n",
    "    eval_fn=make_nn_eval_fn(nn, state_to_nn_input),\n",
    "    num_iterations=int(config[\"alphazero_evaluation\"][\"num_iterations\"]),\n",
    "    max_nodes=int(config[\"alphazero_evaluation\"][\"max_nodes\"]),\n",
    "    temperature=float(config[\"alphazero_evaluation\"][\"temperature\"]),\n",
    "    dirichlet_epsilon=float(config[\"alphazero_evaluation\"][\"dirichlet_epsilon\"]),\n",
    "    branching_factor=env.num_actions,\n",
    "    action_selector=PUCTSelector(c=float(config[\"alphazero_evaluation\"][\"puct_c\"])),\n",
    "    discount=float(config[\"alphazero_evaluation\"][\"discount\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3233b885-2a2d-460e-997b-aad4e7a6c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "batch_size = int(config[\"trainer\"][\"batch_size\"])\n",
    "train_batch_size = int(config[\"trainer\"][\"train_batch_size\"])\n",
    "warmup_steps = int(config[\"trainer\"][\"warmup_steps\"])\n",
    "collection_steps_per_epoch = int(config[\"trainer\"][\"collection_steps_per_epoch\"])\n",
    "train_steps_per_epoch = batch_size * collection_steps_per_epoch // train_batch_size\n",
    "test_num_episodes = config.getint(\"trainer\",\"test_num_episodes\", fallback=100)\n",
    "\n",
    "opt = config.get(\"trainer\", \"optimizer\") \n",
    "if opt ==\"sgd\":\n",
    "    optimizer = optax.sgd\n",
    "elif opt == \"adam\":\n",
    "    optimizer = optax.adam\n",
    "elif opt == \"adamw\":\n",
    "    optimizer = optax.adamw\n",
    "else:\n",
    "    raise TypeError(\"Not a valid optimizer (sgd, adam, adamw)\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    batch_size=batch_size,\n",
    "    train_batch_size=train_batch_size,\n",
    "    warmup_steps=warmup_steps,\n",
    "    collection_steps_per_epoch=M_TARGET_DEPTH*collection_steps_per_epoch,\n",
    "    train_steps_per_epoch=train_steps_per_epoch,\n",
    "    nn=nn,\n",
    "    loss_fn=partial(az_default_loss_fn, l2_reg_lambda=float(config[\"trainer\"][\"l2_reg_lambda\"])),\n",
    "    optimizer=optimizer(float(config[\"trainer\"][\"optimizer_lr\"])),\n",
    "    evaluator=alphazero,\n",
    "    memory_buffer=replay_memory,\n",
    "    max_episode_steps=max_steps,\n",
    "    env_step_fn=step_fn,\n",
    "    env_init_fn=_init_fn,\n",
    "    state_to_nn_input_fn=state_to_nn_input,\n",
    "    testers=[SinglePlayerTester(num_episodes=test_num_episodes)],\n",
    "    evaluator_test=alphazero_test,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b8a3e55-546f-4a0c-a741-3b1a57907182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to ../data/resnet/all2all25-12-04_11h24/\n"
     ]
    }
   ],
   "source": [
    "# Saving\n",
    "\n",
    "# Training state automaticallly saved in train_loop\n",
    "# Making a backup in another directory for re-use\n",
    "bckp_dir = config[\"saving\"][\"bckp_dir\"]\n",
    "if config.getboolean(\"saving\", \"use_date\"):\n",
    "    d = datetime.datetime.today().strftime(\"%y-%m-%d_%Hh%M\")\n",
    "    bckp_dir += d+\"/\"\n",
    "os.makedirs(bckp_dir, exist_ok = True)\n",
    "print(f\"Saving data to {bckp_dir}\")\n",
    "\n",
    "# Save quantum_compilation game config file\n",
    "path_qc = q.__file__.split(\"/\")[:-2]\n",
    "path_qc.append(\"config.ini\")\n",
    "path_qc = \"/\".join(path_qc)\n",
    "shutil.copyfile(path_qc,bckp_dir+\"qc_config.ini\")\n",
    "# Save AlphaZero config file\n",
    "shutil.copyfile(\"./train.ini\",bckp_dir+\"config.ini\")\n",
    "\n",
    "def saving(trainer, output):\n",
    "    shutil.copytree(trainer.ckpt_dir+\"/\"+str(output.cur_epoch-1),bckp_dir+str(output.cur_epoch-1),dirs_exist_ok=True)\n",
    "\n",
    "    # Saving other relevant objects to continue training\n",
    "    with open(bckp_dir+'collection.pickle', 'wb') as file:\n",
    "        pickle.dump(output.collection_state, file)\n",
    "    with open(bckp_dir+'test_states.pickle', 'wb') as file:\n",
    "        pickle.dump(output.test_states, file)\n",
    "    with open(bckp_dir+'cur_epoch.pickle', 'wb') as file:\n",
    "        pickle.dump(output.cur_epoch, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93693e67-2c58-4c29-a420-9e15aed18358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading\n",
    "# Load relevant objects\n",
    "def loading() -> TrainLoopOutput:\n",
    "    with open(bckp_dir+'collection.pickle', 'rb') as f:\n",
    "        collection_state = pickle.load(f)\n",
    "    with open(bckp_dir+'test_states.pickle', 'rb') as f:\n",
    "        # Serialize and save the object to the file\n",
    "        test_states = pickle.load(f)\n",
    "    with open(bckp_dir+'cur_epoch.pickle', 'rb') as f:\n",
    "        # Serialize and save the object to the file\n",
    "        cur_epoch = pickle.load(f)\n",
    "\n",
    "    # Restore backup train state\n",
    "    # Copy backed up checkpoint to ckpt_dir\n",
    "    shutil.copytree(bckp_dir+str(cur_epoch-1),trainer.ckpt_dir+\"/\"+str(cur_epoch-1),dirs_exist_ok=True)\n",
    "    # Load train_state\n",
    "    train_state = trainer.load_train_state_from_checkpoint(trainer.ckpt_dir, cur_epoch-1)\n",
    "    \n",
    "    # Build a TrainLoopOutput\n",
    "    init_state = TrainLoopOutput(\n",
    "        collection_state=collection_state,\n",
    "        train_state=train_state,\n",
    "        test_states=test_states,\n",
    "        cur_epoch=cur_epoch)\n",
    "    return init_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91b7763-22f6-4163-bd1a-e828917f5af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean target depth: 5\n"
     ]
    }
   ],
   "source": [
    "# First Epoch\n",
    "num_epochs = int(config[\"trainer\"][\"num_epochs\"])\n",
    "print(f\"Mean target depth: {M_TARGET_DEPTH}\")\n",
    "output = trainer.train_loop(seed=0, num_epochs=num_epochs)\n",
    "saving(trainer, output)\n",
    "init_state = loading()\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3997660a-b5b3-45d8-b7b6-37176a289ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rest of the epoch\n",
    "\n",
    "# Increasing M_TARGET_DEPTH, fine-tuning everytime\n",
    "for i in range(\n",
    "        int(config[\"environment\"][\"init_m_target_depth\"])+1, \n",
    "        int(config[\"environment\"][\"final_m_target_depth\"]),\n",
    "        int(config[\"environment\"][\"target_depth_increment\"])):\n",
    "    k+=1\n",
    "    M_TARGET_DEPTH = i\n",
    "    print(f\"Mean target depth: {M_TARGET_DEPTH}\")\n",
    "    trainer = Trainer(\n",
    "        batch_size = batch_size, # number of parallel environments to collect self-play games from\n",
    "        train_batch_size = train_batch_size, # training minibatch size\n",
    "        warmup_steps = 0, #non need as we re-load the collection\n",
    "        collection_steps_per_epoch = M_TARGET_DEPTH*collection_steps_per_epoch,\n",
    "        train_steps_per_epoch = M_TARGET_DEPTH*train_steps_per_epoch,\n",
    "        nn = nn,\n",
    "        loss_fn = partial(az_default_loss_fn, l2_reg_lambda = float(config[\"trainer\"][\"l2_reg_lambda\"])),\n",
    "        optimizer = optimizer(float(config[\"trainer\"][\"optimizer_lr\"])),\n",
    "        evaluator = alphazero,\n",
    "        memory_buffer = replay_memory,\n",
    "        max_episode_steps = max_steps,\n",
    "        env_step_fn = step_fn,\n",
    "        env_init_fn = _init_fn,\n",
    "        state_to_nn_input_fn=state_to_nn_input,\n",
    "        testers=[SinglePlayerTester(num_episodes=test_num_episodes)],\n",
    "        evaluator_test = alphazero_test,\n",
    "        # wandb_project_name='weighted_mcts_test' \n",
    "    )\n",
    "    output_continued = trainer.train_loop(seed=0, num_epochs=num_epochs*k, initial_state=init_state);\n",
    "    saving(trainer, output_continued)\n",
    "    init_state = loading()\n",
    "\n",
    "# Keep training at constant (max) target depth\n",
    "k+=1\n",
    "trainer = Trainer(\n",
    "    batch_size = batch_size, # number of parallel environments to collect self-play games from\n",
    "    train_batch_size = train_batch_size, # training minibatch size\n",
    "    warmup_steps = 0, #non need as we re-load the collection\n",
    "    collection_steps_per_epoch = collection_steps_per_epoch*M_TARGET_DEPTH,\n",
    "    train_steps_per_epoch = M_TARGET_DEPTH*train_steps_per_epoch,\n",
    "    nn = nn,\n",
    "    loss_fn = partial(az_default_loss_fn, l2_reg_lambda = float(config[\"trainer\"][\"l2_reg_lambda\"])),\n",
    "    optimizer = optimizer(float(config[\"trainer\"][\"optimizer_lr\"])),\n",
    "    evaluator = alphazero,\n",
    "    memory_buffer = replay_memory,\n",
    "    max_episode_steps = max_steps,\n",
    "    env_step_fn = step_fn,\n",
    "    env_init_fn = _init_fn,\n",
    "    state_to_nn_input_fn=state_to_nn_input,\n",
    "    testers=[SinglePlayerTester(num_episodes=test_num_episodes)],\n",
    "    evaluator_test = alphazero_test,\n",
    "    # wandb_project_name='weighted_mcts_test' \n",
    ")\n",
    "output_continued = trainer.train_loop(seed=0, num_epochs=config.getint(\"trainer\",\"extra_epochs\")+num_epochs*k, initial_state=init_state);\n",
    "saving(trainer, output_continued)\n",
    "init_state = loading()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
